{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Source Citation** https://keras.io/examples/nlp/pretrained_word_embeddings/<br>\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/05/05<br>\n",
    "**Last modified:** 2020/05/05<br>\n",
    "**Description:** Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "### print(\"Number of directories:\", len(dirnames))\n",
    "### print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "### print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "### print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    ### print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "### print(\"Classes:\", class_names)\n",
    "### print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "##voc = vectorizer.get_vocabulary()\n",
    "##word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(button):\n",
    "    print(\"file: \", file)\n",
    "    path_to_glove_file = os.path.join(\n",
    "        os.path.expanduser(\"~\"), \"NMHU\\\\BSSD5350\\\\Lessons\\\\L06\\\\glove.6B\\\\\" + file.value + \".txt\"\n",
    "    )\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(path_to_glove_file, encoding=\"cp437\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "    num_tokens = len(voc) + 2\n",
    "    if file.value == 'glove.6B.50d':\n",
    "        dim = 50\n",
    "    elif file.value == 'glove.6B.100d':\n",
    "        dim = 100\n",
    "    elif file.value == 'glove.6B.200d':\n",
    "        dim = 200\n",
    "    elif file.value == 'glove.6B.300d':\n",
    "        dim = 300\n",
    "    else:\n",
    "        dim = 50\n",
    "    embedding_dim = dim\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "    from tensorflow.keras.layers import Embedding\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "    x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "    x = layers.MaxPooling1D(5)(x)\n",
    "    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling1D(5)(x)\n",
    "    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    model.summary()\n",
    "\n",
    "    x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "    x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "    y_train = np.array(train_labels)\n",
    "    y_val = np.array(val_labels)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    "    )\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "    #you can copy the save and load from notebook 6.1\n",
    "    #but change theoutput filename\n",
    "    model.save(\"glove-newsgroups\")\n",
    "    #this created a 126.4 MB fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6729f2e86a4adfbb5d356cba3fd19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose a file:', options=('glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.30â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22b4870a6734aaf9151583e62cad6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Train', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.200d\n",
      "file:  Dropdown(description='Choose a file:', index=2, options=('glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d'), value='glove.6B.200d')\n",
      "Found 400000 word vectors.\n",
      "Converted 18018 words (1982 misses)\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 200)         4000400   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 128)         128128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 4,311,716\n",
      "Trainable params: 311,316\n",
      "Non-trainable params: 4,000,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 2.6197 - acc: 0.1457 - val_loss: 1.9112 - val_acc: 0.3608\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 1.7535 - acc: 0.4015 - val_loss: 1.3862 - val_acc: 0.5261\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 18s 148ms/step - loss: 1.3201 - acc: 0.5427 - val_loss: 1.1510 - val_acc: 0.6054\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 1.0950 - acc: 0.6265 - val_loss: 1.1167 - val_acc: 0.6242\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 16s 130ms/step - loss: 0.9341 - acc: 0.6823 - val_loss: 1.0267 - val_acc: 0.6574\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 16s 125ms/step - loss: 0.7935 - acc: 0.7248 - val_loss: 1.0139 - val_acc: 0.6719\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 16s 126ms/step - loss: 0.6750 - acc: 0.7650 - val_loss: 1.1121 - val_acc: 0.6584\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 16s 130ms/step - loss: 0.5750 - acc: 0.8000 - val_loss: 0.8805 - val_acc: 0.7252\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.4813 - acc: 0.8339 - val_loss: 0.9160 - val_acc: 0.7234\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.4049 - acc: 0.8603 - val_loss: 0.9199 - val_acc: 0.7407\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.3513 - acc: 0.8839 - val_loss: 0.9911 - val_acc: 0.7237\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.2987 - acc: 0.9002 - val_loss: 1.0275 - val_acc: 0.7249\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.2507 - acc: 0.9179 - val_loss: 1.1992 - val_acc: 0.7089\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.2295 - acc: 0.9221 - val_loss: 1.1017 - val_acc: 0.7459\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.2064 - acc: 0.9312 - val_loss: 1.1562 - val_acc: 0.7382\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 18s 142ms/step - loss: 0.1792 - acc: 0.9382 - val_loss: 1.1259 - val_acc: 0.7402\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.1734 - acc: 0.9417 - val_loss: 1.1611 - val_acc: 0.7514\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 16s 132ms/step - loss: 0.1585 - acc: 0.9463 - val_loss: 1.2846 - val_acc: 0.7409\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.1536 - acc: 0.9481 - val_loss: 1.4512 - val_acc: 0.7292\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.1478 - acc: 0.9521 - val_loss: 1.3312 - val_acc: 0.7417\n",
      "WARNING:tensorflow:From C:\\Users\\sbing\\.conda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\sbing\\.conda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: glove-newsgroups\\assets\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, Button\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "def dropdown_eventhandler(change):\n",
    "    print(change.new)\n",
    "\n",
    "#define dropdown\n",
    "option_list = ('glove.6B.50d','glove.6B.100d','glove.6B.200d','glove.6B.300d')\n",
    "file = Dropdown(description=\"Choose a file:\", options=option_list)\n",
    "file.observe(dropdown_eventhandler, names='value')\n",
    "display(file)\n",
    "\n",
    "# define button\n",
    "btn=Button(description=\"Train\")\n",
    "display(btn)\n",
    "btn.on_click(train_model) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pretrained_word_embeddings",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
